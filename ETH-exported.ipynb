{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from web3 import Web3\n",
    "import datetime\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Function to read in YAML file\n",
    "def read_yaml_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "    \n",
    "# read in all yaml files\n",
    "token_addresses = read_yaml_file('token-addresses.yml')\n",
    "ethereum_token_addresses = token_addresses['ethereum']\n",
    "track_bridged_eth = read_yaml_file('track-bridged-eth.yml')\n",
    "\n",
    "# Connect to Ethereum node\n",
    "rpc_url = 'https://eth.llamarpc.com'\n",
    "rpc_url = 'https://ethereum-rpc.publicnode.com'\n",
    "rpc_url = 'https://eth-mainnet.public.blastapi.io'\n",
    "rpc_url = 'https://mainnet.gateway.tenderly.co'\n",
    "w3 = Web3(Web3.HTTPProvider(rpc_url))\n",
    "\n",
    "# Function to get Ethereum balance for each address\n",
    "def get_eth_balance(w3: Web3, address, at_block='latest'):\n",
    "    try:\n",
    "        balance = w3.eth.get_balance(Web3.to_checksum_address(address), block_identifier=at_block) / 10**18\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving balance for {address}: {e}\")\n",
    "    return balance\n",
    "\n",
    "# Function to get ERC20 balance of a given coin & address\n",
    "def get_erc20_balance_ethereum(w3: Web3, coin, address, at_block='latest'):\n",
    "    contract_address = ethereum_token_addresses[coin]['contract']\n",
    "    ABI = ethereum_token_addresses[coin]['abi']\n",
    "    try:\n",
    "        contract = w3.eth.contract(address=Web3.to_checksum_address(contract_address), abi=ABI)\n",
    "        balance = contract.functions.balanceOf(Web3.to_checksum_address(address)).call(block_identifier=at_block) / 10**18\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving balance for {address}: {e}\")\n",
    "    return balance\n",
    "\n",
    "# Function to get prices of wstETH, we assume 1stETH = 1 ETH\n",
    "def get_ETH_wstETH_price(w3: Web3, at_block='latest'):\n",
    "    address = ethereum_token_addresses['wstETH']['contract']\n",
    "    abi = ethereum_token_addresses['wstETH']['abi']\n",
    "    try:\n",
    "        contract = w3.eth.contract(address=Web3.to_checksum_address(address), abi=abi)\n",
    "        price = contract.functions.stEthPerToken().call(block_identifier=at_block) / 10**18\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving price for wstETH: {e}\")\n",
    "    return price\n",
    "\n",
    "# Function to get price of mETH\n",
    "def get_ETH_mETH_price(w3: Web3, at_block='latest'):\n",
    "    address = ethereum_token_addresses['mETH']['staking_contract']\n",
    "    abi = ethereum_token_addresses['mETH']['staking_abi']\n",
    "    try:\n",
    "        contract = w3.eth.contract(address=Web3.to_checksum_address(address), abi=abi)\n",
    "        price = contract.functions.mETHToETH(10**18).call(block_identifier=at_block) / 10**18\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving price for mETH: {e}\")\n",
    "    return price\n",
    "\n",
    "# Function to get price of pufETH\n",
    "def get_ETH_pufETH_price(w3: Web3, at_block='latest'):\n",
    "    address = ethereum_token_addresses['pufETH']['contract']\n",
    "    abi = ethereum_token_addresses['pufETH']['abi']\n",
    "    try:\n",
    "        contract = w3.eth.contract(address=Web3.to_checksum_address(address), abi=abi)\n",
    "        price = contract.functions.convertToAssets(10**18).call(block_identifier=at_block) / 10**18\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving price for pufETH: {e}\")\n",
    "    return price\n",
    "\n",
    "# Function to get price of weETH, assuming 1 eETH = 1 ETH\n",
    "def get_ETH_weETH_price(w3: Web3, at_block='latest'):\n",
    "    address = ethereum_token_addresses['weETH']['contract']\n",
    "    abi = ethereum_token_addresses['weETH']['abi']\n",
    "    try:\n",
    "        contract = w3.eth.contract(address=Web3.to_checksum_address(address), abi=abi)\n",
    "        price = contract.functions.getEETHByWeETH(10**18).call(block_identifier=at_block) / 10**18\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving price for weETH: {e}\")\n",
    "    return price\n",
    "\n",
    "# Function to get price of rETH\n",
    "def get_ETH_rETH_price(w3: Web3, at_block='latest'):\n",
    "    address = ethereum_token_addresses['rETH']['contract']\n",
    "    abi = ethereum_token_addresses['rETH']['abi']\n",
    "    try:\n",
    "        contract = w3.eth.contract(address=Web3.to_checksum_address(address), abi=abi)\n",
    "        price = contract.functions.getExchangeRate().call(block_identifier=at_block) / 10**18\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving price for rETH: {e}\")\n",
    "    return price\n",
    "\n",
    "# Function to get price of cbETH\n",
    "def get_ETH_cbETH_price(w3: Web3, at_block='latest'):\n",
    "    address = ethereum_token_addresses['cbETH']['contract']\n",
    "    abi = ethereum_token_addresses['cbETH']['abi']\n",
    "    try:\n",
    "        contract = w3.eth.contract(address=Web3.to_checksum_address(address), abi=abi)\n",
    "        price = contract.functions.exchangeRate().call(block_identifier=at_block) / 10**18\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving price for cbETH: {e}\")\n",
    "    return price\n",
    "\n",
    "# Function to get the block number of the first block of the day using binary search\n",
    "def get_first_block_of_day(w3: Web3, target_date: datetime.date):\n",
    "    # Convert the target date to a UNIX timestamp (00:00:00 of that day in UTC)\n",
    "    start_of_day = datetime.datetime.combine(target_date, datetime.time(0, 0), tzinfo=datetime.timezone.utc)\n",
    "    start_timestamp = int(start_of_day.timestamp())\n",
    "\n",
    "    # Get the current block number (latest block)\n",
    "    latest_block = w3.eth.get_block('latest')['number']\n",
    "\n",
    "    # Perform binary search to find the block with the timestamp >= start_timestamp\n",
    "    low = 0\n",
    "    high = latest_block\n",
    "\n",
    "    while low < high:\n",
    "        mid = (low + high) // 2\n",
    "        mid_block = w3.eth.get_block(mid)\n",
    "        if mid_block['timestamp'] < start_timestamp:\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid\n",
    "\n",
    "    # After the search, 'low' should be the first block of the day or a block at or after the target timestamp\n",
    "    first_block_of_day = w3.eth.get_block(low)\n",
    "\n",
    "    return first_block_of_day if first_block_of_day['timestamp'] >= start_timestamp else None\n",
    "\n",
    "# Function to lookup the block number of a given date using block_timestamps.csv\n",
    "def lookup_block_number(date: str):\n",
    "    df = pd.read_csv('block_timestamps.csv')\n",
    "    try:\n",
    "        block_number = int(df.loc[df['date'] == date, 'block'].values[0])\n",
    "    except:\n",
    "        block_number = None\n",
    "    return block_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to backfill block_timestamps.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the last date in the csv\n",
    "df = pd.read_csv('block_timestamps.csv')\n",
    "last_date = pd.to_datetime(df['date'].iloc[-1]).date()\n",
    "\n",
    "# get current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "while current_date > last_date:\n",
    "    # get block number\n",
    "    new_block = get_first_block_of_day(w3, last_date + datetime.timedelta(days=1))\n",
    "    new_row = pd.DataFrame({'date': [str(last_date + datetime.timedelta(days=1))], \n",
    "                            'block': [new_block['number']], \n",
    "                            'block_timestamp': [new_block['timestamp']]})\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "    last_date = last_date + datetime.timedelta(days=1)\n",
    "    # save to csv\n",
    "    df.to_csv('block_timestamps.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to create folder structure and empty csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an empty dataframe\n",
    "df = pd.read_csv('block_timestamps.csv')\n",
    "df = df.drop(columns=['block', 'block_timestamp'])\n",
    "\n",
    "# read in all yaml files\n",
    "token_addresses = read_yaml_file('token-addresses.yml')\n",
    "ethereum_token_addresses = token_addresses['ethereum']\n",
    "track_bridged_eth = read_yaml_file('track-bridged-eth.yml')\n",
    "\n",
    "# create holdings folder\n",
    "if os.path.exists('holdings') == False:\n",
    "    os.mkdir('holdings')\n",
    "\n",
    "# create a dictionary for each entity\n",
    "for entity in list(track_bridged_eth):\n",
    "    if os.path.exists(f\"holdings/{entity}\") == False:\n",
    "        os.mkdir(f\"holdings/{entity}\")\n",
    "\n",
    "# create subfolder for each chain in entity\n",
    "for entity in list(track_bridged_eth):\n",
    "    for chain in list(track_bridged_eth[entity]):\n",
    "        if os.path.exists(f\"holdings/{entity}/{chain}\") == False:\n",
    "            os.mkdir(f\"holdings/{entity}/{chain}\")\n",
    "\n",
    "# create a csv file for each token in each chain\n",
    "for entity in list(track_bridged_eth):\n",
    "    for chain in list(track_bridged_eth[entity]):\n",
    "        for token in [list(item.keys())[0] for item in track_bridged_eth[entity][chain]]:\n",
    "            if os.path.exists(f\"holdings/{entity}/{chain}/{token}.csv\") == False:\n",
    "                df.to_csv(f\"holdings/{entity}/{chain}/{token}.csv\", index=False)\n",
    "\n",
    "# create prices folder\n",
    "if os.path.exists('prices') == False:\n",
    "    os.mkdir('prices')\n",
    "\n",
    "# create a folder for each chain in prices folder\n",
    "for chain in list(token_addresses.keys()):\n",
    "    if os.path.exists(f\"prices/{chain}\") == False:\n",
    "        os.mkdir(f\"prices/{chain}\")\n",
    "\n",
    "# create a csv file for each token in each chain\n",
    "for chain in list(token_addresses.keys()):\n",
    "    for token in list(token_addresses[chain]):\n",
    "        if os.path.exists(f\"prices/{chain}/{token}.csv\") == False:\n",
    "            df.to_csv(f\"prices/{chain}/{token}.csv\", index=False)\n",
    "\n",
    "\n",
    "### extend the csv files with the new dates\n",
    "\n",
    "for entity in list(track_bridged_eth):\n",
    "    for chain in list(track_bridged_eth[entity]):\n",
    "        for token in [list(item.keys())[0] for item in track_bridged_eth[entity][chain]]:\n",
    "            df = pd.read_csv(f\"holdings/{entity}/{chain}/{token}.csv\")\n",
    "            last_date = pd.to_datetime(df['date'].iloc[-1]).date()\n",
    "            column_names = df.columns\n",
    "            for date in pd.date_range(start=last_date + datetime.timedelta(days=1), end=current_date):\n",
    "                new_row = pd.DataFrame({'date': [date.strftime('%Y-%m-%d')]}, columns=column_names)\n",
    "                new_row.to_csv(f\"holdings/{entity}/{chain}/{token}.csv\", index=False, mode='a', header=False)\n",
    "\n",
    "for chain in list(token_addresses.keys()):\n",
    "    for token in list(token_addresses[chain]):\n",
    "        df = pd.read_csv(f\"prices/{chain}/{token}.csv\")\n",
    "        last_date = pd.to_datetime(df['date'].iloc[-1]).date()\n",
    "        column_names = df.columns\n",
    "        for date in pd.date_range(start=last_date + datetime.timedelta(days=1), end=current_date):\n",
    "            new_row = pd.DataFrame({'date': [date.strftime('%Y-%m-%d')]}, columns=column_names)\n",
    "            new_row.to_csv(f\"prices/{chain}/{token}.csv\", index=False, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to backfill holdings on Ethereum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Beacon_chain_deposits\n",
      "Skipping 0x00000000219ab540356cbb839cbe05303d7705fa as already filled\n",
      "Processing Arbitrum\n",
      "Skipping 0x8315177ab297ba92a06054ce80a67ed4dbd7ed3a as already filled\n",
      "Skipping 0x011B6E24FfB0B5f5fCc564cf4183C5BBBc96D515 as already filled\n",
      "Skipping 0x0F25c1DC2a9922304f2eac71DCa9B07E310e8E5a as already filled\n",
      "Processing Base\n",
      "Skipping 0x49048044d57e1c92a77f79988d21fa8faf74e97e as already filled\n",
      "Skipping 0x9de443AdC5A411E83F1878Ef24C3F52C61571e72 as already filled\n",
      "Skipping 0x3154Cf16ccdb4C6d922629664174b904d80F2C35 as already filled\n",
      "Processing Optimism\n",
      "Skipping 0xbEb5Fc579115071764c7423A4f12eDde41f106Ed as already filled\n",
      "Skipping 0x99C9fc46f92E8a1c0deC1b1747d010903E884bE1 as already filled\n",
      "Skipping 0x76943C0D61395d8F2edF9060e1533529cAe05dE6 as already filled\n",
      "Processing Scroll\n",
      "Skipping 0x6774Bcbd5ceCeF1336b5300fb5186a12DDD8b367 as already filled\n",
      "Skipping 0xA033Ff09f2da45f0e9ae495f525363722Df42b2a as already filled\n",
      "Skipping 0x6625C6332c9F91F2D27c304E729B86db87A3f504 as already filled\n",
      "Processing Blast\n",
      "Skipping 0x98078db053902644191f93988341E31289E1C8FE as already filled\n",
      "Skipping 0x5F6AE08B8AeB7078cf2F96AFb089D7c9f51DA47d as already filled\n",
      "Skipping 0x98078db053902644191f93988341E31289E1C8FE as already filled\n",
      "Processing Mantle\n",
      "Skipping 0xc54cb22944F2bE476E02dECfCD7e3E7d3e15A8Fb as already filled\n",
      "Skipping 0x95fC37A27a2f68e3A647CDc081F0A89bb47c3012 as already filled\n",
      "Skipping 0x95fC37A27a2f68e3A647CDc081F0A89bb47c3012 as already filled\n",
      "Processing Zksync\n",
      "Skipping 0xD7f9f54194C633F36CCD5F3da84ad4a1c38cB2cB as already filled\n",
      "Skipping 0x32400084C286CF3E17e7B677ea9583e60a000324 as already filled\n",
      "Skipping 0x41527B2d03844dB6b0945f25702cB958b6d55989 as already filled\n",
      "Processing Linea\n",
      "Skipping 0xd19d4B5d358258f05D7B411E21A1460D11B0876F as already filled\n",
      "Skipping 0x051F1D88f0aF5763fB888eC4378b4D8B29ea3319 as already filled\n",
      "Processing Starknet\n",
      "Skipping 0xae0Ee0A63A2cE6BaeEFFE56e7714FB4EFE48D419 as already filled\n",
      "Skipping 0xBf67F59D2988A46FBFF7ed79A621778a3Cd3985B as already filled\n",
      "Processing Manta_Pacific\n",
      "Skipping 0x9168765EE952de7C6f8fC6FaD5Ec209B960b7622 as already filled\n",
      "Processing Mode\n",
      "Skipping 0x8B34b14c7c7123459Cf3076b8Cb929BE097d0C07 as already filled\n",
      "Processing Metis\n",
      "Skipping 0x3980c9ed79d2c191A89E02Fa3529C60eD6e9c04b as already filled\n",
      "Processing Polygon_zkevm\n",
      "Skipping 0x2a3DD3EB832aF982ec71669E178424b10Dca2EDe as already filled\n",
      "Processing Polygon_POS\n",
      "Skipping 0x8484Ef722627bf18ca5Ae6BcF031c23E6e922B30 as already filled\n",
      "Skipping 0xa45b966996374E9e65ab991C6FE4Bfce3a56DDe8 as already filled\n",
      "Processing Ronin\n",
      "Skipping 0x64192819Ac13Ef72bF6b5AE239AC672B43a9AF08 as already filled\n",
      "Skipping 0x1A2a1c938CE3eC39b6D47113c7955bAa9DD454F2 as already filled\n",
      "Skipping 0x64192819Ac13Ef72bF6b5AE239AC672B43a9AF08 as already filled\n",
      "Skipping 0x1A2a1c938CE3eC39b6D47113c7955bAa9DD454F2 as already filled\n",
      "Processing Avalanche\n",
      "Skipping 0x8EB8a3b98659Cce290402893d0123abb75E3ab28 as already filled\n",
      "Skipping 0xE78388b4CE79068e89Bf8aA7f218eF6b9AB0e9d0 as already filled\n",
      "Skipping 0x8EB8a3b98659Cce290402893d0123abb75E3ab28 as already filled\n",
      "Skipping 0xE78388b4CE79068e89Bf8aA7f218eF6b9AB0e9d0 as already filled\n",
      "Processing PulseChain\n",
      "Skipping 0x1715a3E4A142d8b698131108995174F37aEBA10D as already filled\n",
      "Processing Aptos\n",
      "Skipping 0x50002CdFe7CCb0C41F519c6Eb0653158d11cd907 as already filled\n",
      "Processing Near\n",
      "Skipping 0x6BFaD42cFC4EfC96f529D786D643Ff4A8B89FA52 as already filled\n",
      "Skipping 0x23Ddd3e3692d1861Ed57EDE224608875809e127f as already filled\n",
      "Processing Gnosis_Chain\n",
      "Skipping 0x88ad09518695c6c3712AC10a214bE5109a655671 as already filled\n",
      "Skipping 0x88ad09518695c6c3712AC10a214bE5109a655671 as already filled\n",
      "Skipping 0x88ad09518695c6c3712AC10a214bE5109a655671 as already filled\n",
      "Processing SUI\n",
      "Skipping 0x312e67b47A2A29AE200184949093D92369F80B53 as already filled\n",
      "Processing Worldchain\n",
      "Skipping 0xd5ec14a83B7d95BE1E2Ac12523e2dEE12Cbeea6C as already filled\n",
      "Processing Sollet\n",
      "Skipping 0xeae57ce9cc1984F202e15e038B964bb8bdF7229a as already filled\n",
      "Processing Wormhole\n",
      "Skipping 0x3ee18B2214AFF97000D974cf647E7C347E8fa585 as already filled\n",
      "Skipping 0xf92cD566Ea4864356C5491c177A430C222d7e678 as already filled\n",
      "Processing Stargate\n",
      "Skipping 0x72E2F4830b9E45d52F80aC08CB2bEC0FeF72eD9c as already filled\n",
      "Skipping 0x77b2043768d28E9C9aB44E1aBfC95944bcE57931 as already filled\n",
      "Skipping 0xA572d137666DCbAdFA47C3fC41F15e90134C618c as already filled\n",
      "Skipping 0x268Ca24DAefF1FaC2ed883c598200CcbB79E931D as already filled\n",
      "Processing Orbiter\n",
      "Skipping 0x80C67432656d59144cEFf962E8fAF8926599bCF8 as already filled\n",
      "Processing Orbit\n",
      "Skipping 0x1Bf68A9d1EaEe7826b3593C20a0ca93293cb489a as already filled\n",
      "Processing Across\n",
      "Skipping 0xc186fA914353c44b2E33eBE05f21846F1048bEda as already filled\n",
      "Skipping 0xc186fA914353c44b2E33eBE05f21846F1048bEda as already filled\n",
      "Skipping 0x5c7BCd6E7De5423a257D81B442095A1a6ced35C5 as already filled\n",
      "Skipping 0x4D9079Bb4165aeb4084c526a32695dCfd2F77381 as already filled\n",
      "Processing RhinoFi\n",
      "Skipping 0x5d22045DAcEAB03B158031eCB7D9d06Fad24609b as already filled\n",
      "Processing Connext\n",
      "Skipping 0x8898B472C54c31894e3B9bb83cEA802a5d0e63C6 as already filled\n",
      "Skipping 0xd44E91CfBBAa7b3B259A12a43b38CEBf47B463D5 as already filled\n",
      "Skipping 0xC8140dA31E6bCa19b287cC35531c2212763C2059 as already filled\n",
      "Processing Hop\n",
      "Skipping 0xb8901acB165ed027E32754E0FFe830802919727f as already filled\n",
      "Processing Fraxferry\n",
      "Skipping 0x505603e2440b44C1602b44D0Eb8385399b3F7bab as already filled\n",
      "Skipping 0x8afd5082E0C24dEcEA39A9eFb14e4ACF4373D7D6 as already filled\n",
      "Processing Harmony\n",
      "Skipping 0xF9Fb1c508Ff49F78b60d3A96dea99Fa5d7F3A8A6 as already filled\n",
      "Processing LayerZero\n",
      "Skipping 0xFE7fe01F8B9A76803aF3750144C2715D9bcf7D0D as already filled\n",
      "Skipping 0x1f55a02A049033E3419a8E2975cF3F572F4e6E9A as already filled\n",
      "Skipping 0x1cd5b73d12CB23b2835C873E4FaFfE83bBCef208 as already filled\n",
      "Skipping 0x85d456B2DfF1fd8245387C0BfB64Dfb700e98Ef3 as already filled\n",
      "Processing Nomad\n",
      "Skipping 0x88A69B4E698A4B090DF6CF5Bd7B2D47325Ad30A3 as already filled\n",
      "Processing Optics\n",
      "Skipping 0x6a39909e805A3eaDd2b61fFf61147796ca6aBB47 as already filled\n",
      "Skipping 0x4fc16De11deAc71E8b2Db539d82d93BE4b486892 as already filled\n",
      "Processing Axler\n",
      "Skipping 0x4F4495243837681061C4743b74B3eEdf548D56A5 as already filled\n",
      "Processing Derive\n",
      "Skipping 0x4BB4C3CDc7562f08e9910A0C7D8bB7e108861eB4 as already filled\n",
      "Skipping 0x8180EcCC825b692ef65FF099a0A387743788bf78 as already filled\n",
      "Skipping 0xD4efe33C66B8CdE33B8896a2126E41e5dB571b7e as already filled\n",
      "Skipping 0xeBB5D642aA8ccDeE98373D6aC3ee0602b63824b3 as already filled\n",
      "Skipping 0x35d4D9bc79B0a543934b1769304B90d752691caD as already filled\n",
      "Processing Kinto\n",
      "Skipping 0xc5d01939Af7Ce9Ffc505F0bb36eFeDde7920f2dc as already filled\n",
      "Skipping 0x0f1b7bd7762662B23486320AA91F30312184f70C as already filled\n",
      "Skipping 0xeB66259d2eBC3ed1d3a98148f6298927d8A36397 as already filled\n",
      "Skipping 0x0f1b7bd7762662B23486320AA91F30312184f70C as already filled\n",
      "Skipping 0x00A0c9d82B95a17Cdf2D46703F2DcA13EB0E8A94 as already filled\n",
      "Skipping 0x0f1b7bd7762662B23486320AA91F30312184f70C as already filled\n",
      "Processing Synapse\n",
      "Skipping 0x2796317b0fF8538F253012862c06787Adfb8cEb6 as already filled\n",
      "Skipping 0x2796317b0fF8538F253012862c06787Adfb8cEb6 as already filled\n"
     ]
    }
   ],
   "source": [
    "# read in all yaml files\n",
    "token_addresses = read_yaml_file('token-addresses.yml')\n",
    "ethereum_token_addresses = token_addresses['ethereum']\n",
    "track_bridged_eth = read_yaml_file('track-bridged-eth.yml')\n",
    "\n",
    "for entity in track_bridged_eth.keys():\n",
    "\n",
    "    print(f\"Processing {entity}\")\n",
    "\n",
    "    for j in track_bridged_eth[entity]['ethereum']:\n",
    "\n",
    "        # create a column for each address if not already in the dataframe\n",
    "        asset = list(j.keys())[0]\n",
    "        addresses = j[asset]\n",
    "        df = pd.read_csv(f'holdings/{entity}/ethereum/{asset}.csv')\n",
    "        for address in addresses[0]['address']:\n",
    "            if address not in df.columns:\n",
    "                df[address] = np.nan\n",
    "\n",
    "        for address in addresses[0]['address']:\n",
    "            # only process if the last row of the column is not yet filled\n",
    "            if np.isnan(df[address].iloc[-1]) == False:\n",
    "                print(f\"Skipping {address} as already filled\")\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"Backfilling {address}\")\n",
    "                # iterate through each row from reverse until we get balance = 0\n",
    "                for i in range(len(df)-1, -1, -1):\n",
    "                    date = df['date'].iloc[i]\n",
    "                    block = lookup_block_number(date)\n",
    "                    #print(f\"Processing {date}\")\n",
    "                    if asset == 'ETH':\n",
    "                        balance = get_eth_balance(w3, address, block)\n",
    "                    else:\n",
    "                        balance = get_erc20_balance_ethereum(w3, asset, address, block)\n",
    "\n",
    "                    if np.isnan(df.loc[i, address]):\n",
    "                        df.loc[i, address] = balance\n",
    "                        if i % 100 == 0:\n",
    "                            df.to_csv(f\"holdings/{entity}/ethereum/{asset}.csv\", index=False)\n",
    "                    else: # stop if we already have a value (full backfilled)\n",
    "                        break\n",
    "\n",
    "                    # stop also if balance is 0 = contract not yet used or deployed / no coins on it\n",
    "                    #if balance == 0:\n",
    "                    #    break\n",
    "\n",
    "                # save to csv\n",
    "                df.to_csv(f\"holdings/{entity}/ethereum/{asset}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to backfill prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ethereum\n",
      "Skipping wETH prices as already filled\n",
      "Skipping stETH prices as already filled\n",
      "Skipping wstETH prices as already filled\n",
      "Skipping mETH prices as already filled\n",
      "Skipping cbETH prices as already filled\n",
      "Skipping pufETH prices as already filled\n",
      "Skipping eETH prices as already filled\n",
      "Skipping weETH prices as already filled\n",
      "Backfilling rETH\n",
      "Error retrieving price for rETH: Could not decode contract function call to getExchangeRate with return data: b'', output_types: ['uint256']\n",
      "Error retrieving price for rETH: cannot access local variable 'price' where it is not associated with a value\n",
      "Assuming date reached where contract not yet deployed or implementation changed.\n"
     ]
    }
   ],
   "source": [
    "# only works for assets on ethereum L1 as of now!\n",
    "\n",
    "for chain in token_addresses:\n",
    "\n",
    "    print(f\"Processing {chain}\")\n",
    "\n",
    "    for token in token_addresses[chain]:\n",
    "\n",
    "        # create a column for each address if not already in the dataframe\n",
    "        df = pd.read_csv(f'prices/{chain}/{token}.csv')\n",
    "        if 'price' not in df.columns:\n",
    "            df['price'] = None\n",
    "            df.to_csv(f\"prices/{chain}/{token}.csv\", index=False)\n",
    "\n",
    "        # only process if the whole column is not yet filled\n",
    "        if df['price'].isnull().all() == False:\n",
    "            print(f\"Skipping {token} prices as already filled\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Backfilling {token}\")\n",
    "            # iterate through each row from reverse until we get price = 0\n",
    "            for i in range(len(df)-1, -1, -1):\n",
    "                date = df['date'].iloc[i]\n",
    "                block = lookup_block_number(date)\n",
    "                try:\n",
    "                    if token == 'wstETH':\n",
    "                        price = get_ETH_wstETH_price(w3, block)\n",
    "                    elif token == 'mETH':\n",
    "                        price = get_ETH_mETH_price(w3, block)\n",
    "                    elif token == 'pufETH':\n",
    "                        price = get_ETH_pufETH_price(w3, block)\n",
    "                    elif token == 'weETH':\n",
    "                        price = get_ETH_weETH_price(w3, block)\n",
    "                    elif token == 'rETH':\n",
    "                        price = get_ETH_rETH_price(w3, block)\n",
    "                    elif token == 'cbETH':\n",
    "                        price = get_ETH_cbETH_price(w3, block)\n",
    "                    # add sfrxETH here\n",
    "                    elif token in ['wETH', 'stETH', 'eETH', 'pxETH', 'frxETH', 'ezETH']: # assuming 1 token = 1 ETH\n",
    "                        price = 1\n",
    "                    else:\n",
    "                        print(f\"Token {token} not supported\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error retrieving price for {token}: {e}\")\n",
    "                    print(\"Assuming date reached where contract not yet deployed or implementation changed.\")\n",
    "                    break\n",
    "                df.loc[i, 'price'] = price\n",
    "\n",
    "                # save to csv\n",
    "                df.to_csv(f\"prices/{chain}/{token}.csv\", index=False)\n",
    "\n",
    "                # stop if balance is 0 = contract not yet used\n",
    "                if price == 0:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to get the total supply over time ...? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etherscan is paid plan only\n",
    "# Coingecko API is free but only estimate\n",
    "# DUNE???\n",
    "\n",
    "import requests\n",
    "\n",
    "# Etherscan API key\n",
    "ETHERSCAN_API_KEY = os.environ['ETHERSCAN_API_KEY']\n",
    "\n",
    "# Function to get ETH total supply at a block\n",
    "def get_eth_supply_at_block(block_number):\n",
    "    url = f'https://api.etherscan.io/api?module=proxy&action=eth_getBlockByNumber&tag={hex(block_number)}&boolean=true&apikey={ETHERSCAN_API_KEY}'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    if 'result' in data:\n",
    "        block_reward = int(data['result']['blockReward'], 16)  # Block reward in Wei\n",
    "        return block_reward / (10 ** 18)  # Convert Wei to ETH\n",
    "    else:\n",
    "        print(f\"Error fetching ETH supply at block {block_number}: {data}\")\n",
    "        return None\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
